{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "0. [References](#Reference)\n",
    "1. [Getting Started](#GettingStarted)\n",
    "2. [Architecture](#Architecture)\n",
    "3. [Masked Convolution](#MaskedConvolution)\n",
    "4. [First Masked Convolution](#FirstMaskedConvolution)\n",
    "5. [Residual Blocks](#ResidualBlocks)\n",
    "    1. [Prelu details](#PRelu)\n",
    "6. [Stacking Residual Blocks](#StackingResidualBlocks)\n",
    "7. [Wrapping Up For Outputs](#WrappingUpForOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"References\"></a>\n",
    "## References\n",
    "\n",
    "This code has been implemented following the steps and instructions provided in this [link](https://israelg99.github.io/2017-02-27-Grayscale-PixelCNN-with-Keras/) which in turn is based on the [paper](https://arxiv.org/pdf/1601.06759.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"GettingStarted\"></a>\n",
    "## Getting Started\n",
    "\n",
    "Keras has two ways of defining models, the Sequential, which is the easiest but limiting way, and the Functional, which is more complex but flexible way.\n",
    "\n",
    "We will use the Functional API because we need that additional flexibility, for example - the Sequential model limits the amount of outputs of the model to 1, but to model RGB channels, we will need 3 output units, one for each channel. As the model gets more complex (e.g Gated PixelCNN) it will become clearer why Functional API is a no-brainer for projects like this.\n",
    "\n",
    "Our input shape(excluding batch) should be: (height, width, channels).\n",
    "More specifically, MNIST (grayscale) input shape looks like this (28, 28, 1) and CIFAR (32, 32, 3).\n",
    "\n",
    "Let’s start simple, we’ll do a PixelCNN for grayscale MNIST first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Architecture\"></a>\n",
    "## Architecture\n",
    "\n",
    "Since the paper focuses on PixelRNN, it fails to provide a clear explanation on how the architecture of PixelCNN should look like, however, it does a good job of describing the big picture, but it is not enough for actually implementing PixelCNN.\n",
    "\n",
    "Here’s the architecture I came up with for grayscale MNIST (with only 1 residual block for simplicity):\n",
    "\n",
    "<img src=https://israelg99.github.io/images/2017-02-27-Grayscale-PixelCNN-with-Keras/model.png>\n",
    "\n",
    "Note that PixelCNN has to preserve the spatial dimension of the input, which is not shown in the graph above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"MaskedConvolution\"></a>\n",
    "## Masked Convolution\n",
    "\n",
    "We already defined our input, and as you can see in the architecture graph, the next layer is a masked convolution, which is the next thing we are going to implement.\n",
    "\n",
    "### How to implement grayscale masks?\n",
    "\n",
    "Here’s a picture for reference:\n",
    "\n",
    "<img src=https://israelg99.github.io/images/2017-02-27-Grayscale-PixelCNN-with-Keras/grayscale_mask_typeA.png>\n",
    "\n",
    "The difference between type A and B masks in grayscale images is that type A also masks the center pixel.\n",
    "Keep in mind that masks for grayscale images are simpler than RGB masks, but we’ll get to RGB masks too.\n",
    "\n",
    "Here’s how we are going to implement masks:\n",
    "\n",
    "1. Create a numpy array of ones in the shape of our convolution weights: (height, width, input_channels, output_channels)\n",
    "2. Zero out all weights to the right and below of the center weights (to block future insight of pixels from flowing, as stated in the paper).\n",
    "3. If the mask type is A, we’ll zero out the center weights too (to block insight of the current pixel as well).\n",
    "4. Multiply the mask with the weights before calculating convolutions.\n",
    "\n",
    "Let’s use the steps above to go ahead and implement a new Keras layer for masked convolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Convolution2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConvolution2D(Convolution2D):\n",
    "    #*args pick up any number of non-keyword arguments\n",
    "    #*kwargs pick up any number of keyword arguments that are actually dictionaries\n",
    "    def __init__(self, *args, mask='B' , n_channels=3, mono=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mask_type = mask\n",
    "\n",
    "        self.mask = None\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "        # Create a numpy array of ones in the shape of our convolution weights.\n",
    "        self.mask = np.ones(self.W_shape)\n",
    "\n",
    "        # We assert the height and width of our convolution to be equal as they should.\n",
    "        assert mask.shape[0] == mask.shape[1]\n",
    "\n",
    "        # Since the height and width are equal, we can use either to represent the size of our convolution.\n",
    "        filter_size = self.mask.shape[0]\n",
    "        filter_center = filter_size / 2\n",
    "\n",
    "        # Zero out all weights below the center.\n",
    "        self.mask[math.ceil(filter_center):] = 0\n",
    "\n",
    "        # Zero out all weights to the right of the center.\n",
    "        self.mask[math.floor(filter_center):, math.ceil(filter_center):] = 0\n",
    "\n",
    "        # If the mask type is 'A', zero out the center weigths too.\n",
    "        if self.mask_type == 'A':\n",
    "            self.mask[math.floor(filter_center), math.floor(filter_center)] = 0\n",
    "\n",
    "        # Convert the numpy mask into a tensor mask.\n",
    "        self.mask = K.variable(self.mask)\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        ''' I just copied the Keras Convolution2D call function so don't worry about all this code.\n",
    "            The only important piece is: self.W * self.mask.\n",
    "            Which multiplies the mask with the weights before calculating convolutions. '''\n",
    "        output = K.conv2d(x, self.W * self.mask, strides=self.subsample,\n",
    "                          border_mode=self.border_mode,\n",
    "                          dim_ordering=self.dim_ordering,\n",
    "                          filter_shape=self.W_shape)\n",
    "        if self.bias:\n",
    "            #Dimension ordering th means the channel dimension (the depth) is at index 1.\n",
    "            #nb_filter is the number of convolutional filters to use.\n",
    "            if self.dim_ordering == 'th':\n",
    "                output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n",
    "            #Dimension ordering th means the channel dimension (the depth) is at index 3.\n",
    "            elif self.dim_ordering == 'tf':\n",
    "                output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n",
    "            else:\n",
    "            #There are no other kind of dimension orderings so any other case would be invalid.\n",
    "                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        # Add the mask type property to the config.\n",
    "        return dict(list(super().get_config().items()) + list({'mask': self.mask_type}.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"FirstMaskedConvolution\"></a>\n",
    "## First Masked Convolution Layer\n",
    "\n",
    "Now that we have masked convolutions implemented, let’s add the first masked convolution to our model(which is practically just an input layer at the moment).\n",
    "\n",
    "According to the paper, the layer after the input is a masked convolution of type A, with a filter size of (7,7) and it has to preserve the spatial dimensions of the input, we’ll use border_mode='same' for that.\n",
    "Note that this layer is the only masked convolution of type A the model will have.\n",
    "\n",
    "Now we should have a simple graph like this: input -> masked_convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (28, 28, 1)\n",
    "filters = 128\n",
    "depth = 6\n",
    "\n",
    "input_img = Input(shape)\n",
    "\n",
    "model = MaskedConvolution2D(filters, 7, 7, mask='A', border_mode='same')(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ResidualBlocks\"></a>\n",
    "## Residual blocks\n",
    "\n",
    "After the first masked convolution the model has a series of residual blocks (The architecture picture above has only 1 residual block).\n",
    "\n",
    "To implement a residual block:\n",
    "\n",
    "1. Take input of shape (height, width, filters).\n",
    "2. Halve the filters with a (1,1) convolution.\n",
    "3. Apply a (3,3) masked convolution of type B.\n",
    "4. Scale the filters back to original with (1,1) convolution.\n",
    "5. Merge the original input with the convolutions.\n",
    "\n",
    "The reason for cutting the filters by half and then scaling back to original is because it is a good way to get a computational boost while not significally reducing model performance.\n",
    "\n",
    "Let’s implement a residual block in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(object):\n",
    "    def __init__(self, filters):\n",
    "        self.filters = filters\n",
    "\n",
    "    def __call__(self, model):\n",
    "        # filters -> filters/2\n",
    "        block = PReLU()(model)\n",
    "        block = Convolution2D(self.filters//2, 1, 1)(block)\n",
    "\n",
    "        # filters/2 3x3 -> filters/2\n",
    "        block = PReLU()(block)\n",
    "        block = MaskedConvolution2D(self.filters//2, 3, 3, border_mode='same')(block)\n",
    "\n",
    "        # filters/2 -> filters\n",
    "        block = PReLU()(block)\n",
    "        block = Convolution2D(self.filters, 1, 1)(block)\n",
    "\n",
    "        # Merge the original input with the convolutions.\n",
    "        return Merge(mode='sum')([model, block])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"PRelu\"></a>\n",
    "### PRelu Layers\n",
    "\n",
    "The parametric rectifier linear unit (pReLU) activation layer applies the transform f(x) = max(0, x) + w * min(0, x) to the input data. The backward pReLU layer computes the values z = y*f'(x), where y is the input gradient computed on the preceding layer, w is the weight of the input argument. and\n",
    "\n",
    "<img src=https://software.intel.com/sites/products/documentation/doclib/daal/daal-user-and-reference-guides/daal_prog_guide/equations/GUID-ADC54AE0-43B8-40CA-BA41-245D3240Bee1.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to stack those residual blocks in our model, so let’s create a simple layer for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockList(object):\n",
    "    def __init__(self, filters, depth):\n",
    "        self.filters = filters\n",
    "        self.depth = depth\n",
    "\n",
    "    def __call__(self, model):\n",
    "        for _ in range(self.depth):\n",
    "            model = ResidualBlock(self.filters)(model)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"StackingResidualBlocks\"></a>\n",
    "## Stacking Residual Blocks\n",
    "\n",
    "Now let’s stack those residual blocks on our model.\n",
    "We also need to add an activation after the stack, because the residual block ends with a convolution, not an activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape = (28, 28, 1)\n",
    "#filters = 128\n",
    "#depth = 6\n",
    "\n",
    "#input_img = Input(shape)\n",
    "\n",
    "#model = MaskedConvolution2D(filters, 7, 7, mask='A', border_mode='same')(input_img)\n",
    "\n",
    "model = ResidualBlockList(filters, depth)\n",
    "model = PReLU()(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"WrappingUpForOutput\"></a>\n",
    "## Wrapping Up For Output\n",
    "\n",
    "As shown in the architecture picture above, the model has additional 2 masked convolutions before output. According to the paper, those 2 masked convolutions are of size (1,1) and of type B.\n",
    "\n",
    "Let’s add those to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape = (28, 28, 1)\n",
    "#filters = 128\n",
    "#depth = 6\n",
    "\n",
    "#input_img = Input(shape)\n",
    "\n",
    "#model = MaskedConvolution2D(filters, 7, 7, mask='A', border_mode='same')(input_img)\n",
    "\n",
    "#model = ResidualBlockList(filters, depth)\n",
    "#model = PReLU()(model)\n",
    "\n",
    "for _ in range(2):\n",
    "    model = MaskedConvolution2D(filters, 1, 1, border_mode='valid')(model)\n",
    "    model = PReLU()(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
