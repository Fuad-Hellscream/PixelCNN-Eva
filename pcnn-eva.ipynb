{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "0. [References](#Reference)\n",
    "1. [Getting Started](#GettingStarted)\n",
    "2. [Architecture](#Architecture)\n",
    "3. [Masked Convolution](#MaskedConvolution)\n",
    "4. [First Masked Convolution](#FirstMaskedConvolution)\n",
    "    1. [Keras.Input](#KerasInput)\n",
    "5. [Residual Blocks](#ResidualBlocks)\n",
    "    1. [Prelu details](#PRelu)\n",
    "6. [Stacking Residual Blocks](#StackingResidualBlocks)\n",
    "7. [Wrapping Up For Outputs](#WrappingUpForOutput)\n",
    "8. [Output](#Output)\n",
    "9. [Compiling the Model](#CompilingTheModel)\n",
    "10. [Training the Model](#TrainingTheModel)\n",
    "    1. [Loading MNIST data with Keras](#LoadingMNIST)\n",
    "    2. [Fit the model to MNIST](#FitMNIST)\n",
    "    3. [Generating MNIST](#GenerateMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"References\"></a>\n",
    "## References\n",
    "\n",
    "This code has been implemented following the steps and instructions provided in this [link](https://israelg99.github.io/2017-02-27-Grayscale-PixelCNN-with-Keras/) which in turn is based on the [paper](https://arxiv.org/pdf/1601.06759.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"GettingStarted\"></a>\n",
    "## Getting Started\n",
    "\n",
    "Keras has two ways of defining models, the Sequential, which is the easiest but limiting way, and the Functional, which is more complex but flexible way.\n",
    "\n",
    "We will use the Functional API because we need that additional flexibility, for example - the Sequential model limits the amount of outputs of the model to 1, but to model RGB channels, we will need 3 output units, one for each channel. As the model gets more complex (e.g Gated PixelCNN) it will become clearer why Functional API is a no-brainer for projects like this.\n",
    "\n",
    "Our input shape(excluding batch) should be: (height, width, channels).\n",
    "More specifically, MNIST (grayscale) input shape looks like this (28, 28, 1) and CIFAR (32, 32, 3).\n",
    "\n",
    "Let’s start simple, we’ll do a PixelCNN for grayscale MNIST first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Architecture\"></a>\n",
    "## Architecture\n",
    "\n",
    "Since the paper focuses on PixelRNN, it fails to provide a clear explanation on how the architecture of PixelCNN should look like, however, it does a good job of describing the big picture, but it is not enough for actually implementing PixelCNN.\n",
    "\n",
    "Here’s the architecture I came up with for grayscale MNIST (with only 1 residual block for simplicity):\n",
    "\n",
    "<img src=https://israelg99.github.io/images/2017-02-27-Grayscale-PixelCNN-with-Keras/model.png>\n",
    "\n",
    "Note that PixelCNN has to preserve the spatial dimension of the input, which is not shown in the graph above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"MaskedConvolution\"></a>\n",
    "## Masked Convolution\n",
    "\n",
    "We already defined our input, and as you can see in the architecture graph, the next layer is a masked convolution, which is the next thing we are going to implement.\n",
    "\n",
    "### How to implement grayscale masks?\n",
    "\n",
    "Here’s a picture for reference:\n",
    "\n",
    "<img src=https://israelg99.github.io/images/2017-02-27-Grayscale-PixelCNN-with-Keras/grayscale_mask_typeA.png>\n",
    "\n",
    "The difference between type A and B masks in grayscale images is that type A also masks the center pixel.\n",
    "Keep in mind that masks for grayscale images are simpler than RGB masks, but we’ll get to RGB masks too.\n",
    "\n",
    "Here’s how we are going to implement masks:\n",
    "\n",
    "1. Create a numpy array of ones in the shape of our convolution weights: (height, width, input_channels, output_channels)\n",
    "2. Zero out all weights to the right and below of the center weights (to block future insight of pixels from flowing, as stated in the paper).\n",
    "3. If the mask type is A, we’ll zero out the center weights too (to block insight of the current pixel as well).\n",
    "4. Multiply the mask with the weights before calculating convolutions.\n",
    "\n",
    "Let’s use the steps above to go ahead and implement a new Keras layer for masked convolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Convolution2D\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConvolution2D(Convolution2D):\n",
    "    #*args pick up any number of non-keyword arguments\n",
    "    #*kwargs pick up any number of keyword arguments that are actually dictionaries\n",
    "    def __init__(self, *args, mask='B' , n_channels=3, mono=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mask_type = mask\n",
    "\n",
    "        self.mask = None\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "        # Create a numpy array of ones in the shape of our convolution weights.\n",
    "        self.mask = np.ones(self.kernel.shape)\n",
    "        #print(self.mask, \"is of type\", type(self.mask))\n",
    "        #self.mask = np.ones(np.array(self.get_weights()).shape)\n",
    "        #self.mask = np.ones((np.array(self.get_weights()).shape), (np.array(self.get_weights()).shape))\n",
    "\n",
    "        # We assert the height and width of our convolution to be equal as they should.\n",
    "        assert self.mask.shape[0] == self.mask.shape[1]\n",
    "\n",
    "        # Since the height and width are equal, we can use either to represent the size of our convolution.\n",
    "        filter_size = self.mask.shape[0]\n",
    "        filter_center = filter_size / 2\n",
    "\n",
    "        # Zero out all weights below the center.\n",
    "        self.mask[math.ceil(filter_center):] = 0\n",
    "\n",
    "        # Zero out all weights to the right of the center.\n",
    "        self.mask[math.floor(filter_center):, math.ceil(filter_center):] = 0\n",
    "\n",
    "        # If the mask type is 'A', zero out the center weigths too.\n",
    "        if self.mask_type == 'A':\n",
    "            self.mask[math.floor(filter_center), math.floor(filter_center)] = 0\n",
    "\n",
    "        # Convert the numpy mask into a tensor mask.\n",
    "        self.mask = K.variable(self.mask)\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        ''' I just copied the Keras Convolution2D call function so don't worry about all this code.\n",
    "            The only important piece is: self.W * self.mask.\n",
    "            Which multiplies the mask with the weights before calculating convolutions. '''\n",
    "        output = K.conv2d(x, self.kernel * self.mask, strides=self.strides,\n",
    "                          padding=self.padding,\n",
    "                          data_format=self.data_format,\n",
    "                          filter_shape=self.kernel.shape)\n",
    "                          \n",
    "        if self.use_bias:\n",
    "            #Dimension ordering th means the channel dimension (the depth) is at index 1.\n",
    "            #nb_filter is the number of convolutional filters to use.\n",
    "            if self.data_format == 'th':\n",
    "                output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n",
    "            #Dimension ordering th means the channel dimension (the depth) is at index 3.\n",
    "            elif self.data_format == 'tf':\n",
    "                output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n",
    "            else:\n",
    "            #There are no other kind of dimension orderings so any other case would be invalid.\n",
    "                raise ValueError('Invalid data_format:', self.data_format)\n",
    "\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        # Add the mask type property to the config.\n",
    "        return dict(list(super().get_config().items()) + list({'mask': self.mask_type}.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"FirstMaskedConvolution\"></a>\n",
    "## First Masked Convolution Layer\n",
    "\n",
    "Now that we have masked convolutions implemented, let’s add the first masked convolution to our model(which is practically just an input layer at the moment).\n",
    "\n",
    "According to the paper, the layer after the input is a masked convolution of type A, with a filter size of (7,7) and it has to preserve the spatial dimensions of the input, we’ll use border_mode='same' for that.\n",
    "Note that this layer is the only masked convolution of type A the model will have.\n",
    "\n",
    "Now we should have a simple graph like this: input -> masked_convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"KerasInput\"></a>\n",
    "### Keras.Input\n",
    "\n",
    "Input() is used to instantiate a Keras tensor.\n",
    "\n",
    "A Keras tensor is a TensorFlow symbolic tensor object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model.\n",
    "\n",
    "For instance, if a, b and c are Keras tensors, it becomes possible to do: model = Model(input=[a, b], output=c)\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "* **shape:** A shape tuple (integers), not including the batch size. For instance, shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors. Elements of this tuple can be None; 'None' elements represent dimensions where the shape is not known.\n",
    "* **batch_size:** optional static batch size (integer).\n",
    "* **name:** An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided.\n",
    "* **dtype:** The data type expected by the input, as a string (float32, float64, int32...)\n",
    "* **sparse:** A boolean specifying whether the placeholder to be created is sparse. Only one of 'ragged' and 'sparse' can be True. Note that, if sparse is False, sparse tensors can still be passed into the input - they will be densified with a default value of 0.\n",
    "* **tensor:** Optional existing tensor to wrap into the Input layer. If set, the layer will not create a placeholder tensor.\n",
    "* **ragged:** A boolean specifying whether the placeholder to be created is ragged. Only one of 'ragged' and 'sparse' can be True. In this case, values of 'None' in the 'shape' argument represent ragged dimensions. For more information about RaggedTensors, see this guide.\n",
    "* **kwargs:** deprecated arguments support. Supports batch_shape and batch_input_shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "It seems that you are using the Keras 2 and you are passing both `kernel_size` and `strides` as integer positional arguments. For safety reasons, this is disallowed. Pass `strides` as a keyword argument instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-acff19a119fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0minput_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaskedConvolution2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-d0d33590e641>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mask, n_channels, mono, *args, **kwargs)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#*kwargs pick up any number of keyword arguments that are actually dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'B'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mobject_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mconv2d_args_preprocessor\u001b[1;34m(args, kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mkwd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                     raise ValueError(\n\u001b[1;32m--> 279\u001b[1;33m                         \u001b[1;34m'It seems that you are using the Keras 2 '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m                         \u001b[1;34m'and you are passing both `kernel_size` and `strides` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m                         \u001b[1;34m'as integer positional arguments. For safety reasons, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: It seems that you are using the Keras 2 and you are passing both `kernel_size` and `strides` as integer positional arguments. For safety reasons, this is disallowed. Pass `strides` as a keyword argument instead."
     ]
    }
   ],
   "source": [
    "shape = (28, 28, 1)\n",
    "filters = 128\n",
    "\n",
    "#Depth required when stacking residual blocks\n",
    "depth = 6\n",
    "\n",
    "input_img = Input(shape)\n",
    "\n",
    "model = MaskedConvolution2D(filters, 7, 7, mask='A', padding='same')(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ResidualBlocks\"></a>\n",
    "## Residual blocks\n",
    "\n",
    "After the first masked convolution the model has a series of residual blocks (The architecture picture above has only 1 residual block).\n",
    "\n",
    "To implement a residual block:\n",
    "\n",
    "1. Take input of shape (height, width, filters).\n",
    "2. Halve the filters with a (1,1) convolution.\n",
    "3. Apply a (3,3) masked convolution of type B.\n",
    "4. Scale the filters back to original with (1,1) convolution.\n",
    "5. Merge the original input with the convolutions.\n",
    "\n",
    "The reason for cutting the filters by half and then scaling back to original is because it is a good way to get a computational boost while not significally reducing model performance.\n",
    "\n",
    "Let’s implement a residual block in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(object):\n",
    "    def __init__(self, filters):\n",
    "        self.filters = filters\n",
    "\n",
    "    def __call__(self, model):\n",
    "        # filters -> filters/2\n",
    "        block = PReLU()(model)\n",
    "        block = Convolution2D(self.filters//2, 1, 1)(block)\n",
    "\n",
    "        # filters/2 3x3 -> filters/2\n",
    "        block = PReLU()(block)\n",
    "        block = MaskedConvolution2D(self.filters//2, 3, 3, border_mode='same')(block)\n",
    "\n",
    "        # filters/2 -> filters\n",
    "        block = PReLU()(block)\n",
    "        block = Convolution2D(self.filters, 1, 1)(block)\n",
    "\n",
    "        # Merge the original input with the convolutions.\n",
    "        return Merge(mode='sum')([model, block])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"PRelu\"></a>\n",
    "### PRelu Layers\n",
    "\n",
    "The parametric rectifier linear unit (pReLU) activation layer applies the transform f(x) = max(0, x) + w * min(0, x) to the input data. The backward pReLU layer computes the values z = y*f'(x), where y is the input gradient computed on the preceding layer, w is the weight of the input argument. and\n",
    "\n",
    "<img src=https://software.intel.com/sites/products/documentation/doclib/daal/daal-user-and-reference-guides/daal_prog_guide/equations/GUID-ADC54AE0-43B8-40CA-BA41-245D3240Bee1.png>\n",
    "\n",
    "#### Parameters\n",
    "* **alpha_initializer:** Initializer function for the weights.\n",
    "* **alpha_regularizer:** Regularizer for the weights.\n",
    "* **alpha_constraint:** Constraint for the weights.\n",
    "* **shared_axes:** The axes along which to share learnable parameters for the activation function. For example, if the incoming feature maps are from a 2D convolution with output shape (batch, height, width, channels), and you wish to share parameters across space so that each filter only has one set of parameters, set shared_axes=[1, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to stack those residual blocks in our model, so let’s create a simple layer for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockList(object):\n",
    "    def __init__(self, filters, depth):\n",
    "        self.filters = filters\n",
    "        self.depth = depth\n",
    "\n",
    "    def __call__(self, model):\n",
    "        for _ in range(self.depth):\n",
    "            model = ResidualBlock(self.filters)(model)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"StackingResidualBlocks\"></a>\n",
    "## Stacking Residual Blocks\n",
    "\n",
    "Now let’s stack those residual blocks on our model.\n",
    "We also need to add an activation after the stack, because the residual block ends with a convolution, not an activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape = (28, 28, 1)\n",
    "#filters = 128\n",
    "#depth = 6\n",
    "\n",
    "#input_img = Input(shape)\n",
    "\n",
    "#model = MaskedConvolution2D(filters, 7, 7, mask='A', border_mode='same')(input_img)\n",
    "\n",
    "model = ResidualBlockList(filters, depth)\n",
    "model = PReLU()(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"WrappingUpForOutput\"></a>\n",
    "## Wrapping Up For Output\n",
    "\n",
    "As shown in the architecture picture above, the model has additional 2 masked convolutions before output. According to the paper, those 2 masked convolutions are of size (1,1) and of type B.\n",
    "\n",
    "Let’s add those to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape = (28, 28, 1)\n",
    "#filters = 128\n",
    "#depth = 6\n",
    "\n",
    "#input_img = Input(shape)\n",
    "\n",
    "#model = MaskedConvolution2D(filters, 7, 7, mask='A', border_mode='same')(input_img)\n",
    "\n",
    "#model = ResidualBlockList(filters, depth)\n",
    "#model = PReLU()(model)\n",
    "\n",
    "for _ in range(2):\n",
    "    model = MaskedConvolution2D(filters, 1, 1, border_mode='valid')(model)\n",
    "    model = PReLU()(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Output\"></a>\n",
    "## Output\n",
    "\n",
    "Since we have just one channel, we can convolve the pixels with a convolution of size (1,1) with a single filter and then sigmoid its output.\n",
    "The output of the sigmoid should be a 2D array with an exact shape as the input ((28, 28, 1) for MNIST), with each point in the 2D array representing a (grayscale) color value.\n",
    "\n",
    "It shoud look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape = (28, 28, 1)\n",
    "#filters = 128\n",
    "#depth = 6\n",
    "\n",
    "#input_img = Input(shape)\n",
    "\n",
    "#model = MaskedConvolution2D(filters, 7, 7, mask='A', border_mode='same')(input_img)\n",
    "\n",
    "#model = ResidualBlockList(filters, depth)\n",
    "#model = PReLU()(model)\n",
    "\n",
    "#for _ in range(2):\n",
    "#    model = MaskedConvolution2D(filters, 1, 1, border_mode='valid')(model)\n",
    "#    model = PReLU()(model)\n",
    "\n",
    "outs = Convolution2D(1, 1, 1, border_mode='valid')(model)\n",
    "outs = Activation('sigmoid')(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CompilingTheModel\"></a>\n",
    "## Compiling the Model\n",
    "\n",
    "I chose Nadam quite arbitrarily, you can go with any optimizer you like.\n",
    "Since we use sigmoid in our output activations our loss should be binary_crossentropy.\n",
    "\n",
    "Let’s compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_img, outs)\n",
    "model.compile(optimizer=Nadam(), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TrainingTheModel\"></a>\n",
    "## Training the Model\n",
    "<a id=\"LoadingMNIST\"></a>\n",
    "#### Loading MNIST data with Keras\n",
    "\n",
    "Let’s load MNIST data, change data type to float and normalize it.\n",
    "We ignore the labels of MNIST because they are not useful for our case, our model is not a classifier.\n",
    "\n",
    "Note that I concatenate the training and test data, I do that to have more data to help with training, however, if you need validation data, feel free to not concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, _), (test, _) = mnist.load_data()\n",
    "data = np.concatenate((train, test), axis=0)\n",
    "data = data.astype('float32')\n",
    "data /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"FitMNIST\"></a>\n",
    "#### Fit the Model to MNIST\n",
    "\n",
    "Set the arguments needed for training, I also added a TensorBoard and a ModelCheckpoint callbacks to the training routine.\n",
    "We pass the data both as our input and target output.\n",
    "\n",
    "This setup is similar to an autoencoder’s one, but PixelCNN is not an autoencoder, it doesn’t learn an effcient encoding of the data but rather learns the distribution of the values in the data.\n",
    "\n",
    "Here’s how the fit routine should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 200\n",
    "callbacks = [TensorBoard(), ModelCheckpoint('model.h5')]\n",
    "\n",
    "model.fit(data, data,\n",
    "      batch_size=batch_size, nb_epoch=epochs,\n",
    "      callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"GenerateMNIST\"></a>\n",
    "#### Generating MNIST\n",
    "\n",
    "To generate images PixelCNN needs to predict each pixel separately, more concretly, you will need to feed it an 2D array of zeros, let it predict the first pixel, refeed the array, predict second pixel, and so on…\n",
    "\n",
    "PixelCNN outputs an array of pixel value probabilities for each pixel, to generate different images we should not use argmax when picking a pixel value, but rather pick a pixel value using the probabilities themselves.\n",
    "\n",
    "Keep in mind, generating images does not benefit from the convolution’s concurrent nature and everything has to be done sequentially pixel by pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a batch size allows us to predict a few pixels concurrently.\n",
    "batch = 8\n",
    "\n",
    "# Create an empty array of pixels.\n",
    "pixels = np.zeros(shape=(batch,) + (model.input_shape)[1:])\n",
    "batch, rows, cols, channels = pixels.shape\n",
    "\n",
    "# Iterate the pixels because generation has to be done sequentially pixel by pixel.\n",
    "for row in range(rows):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            # Feed the whole array and retrieving the pixel value probabilities for the next pixel.\n",
    "            ps = model.predict_on_batch(pixels)[channel][:, row*cols+col]\n",
    "\n",
    "            # Use the probabilities to pick a pixel value.\n",
    "            # Lastly, we normalize the value.\n",
    "            pixels[:, row, col, channel] = np.array([np.random.choice(256, p=p) for p in ps]) / 255\n",
    "\n",
    "# Iterate the generated images and plot them with matplotlib.\n",
    "for pic in pixels:\n",
    "    plt.imshow(pic, interpolation='nearest')\n",
    "    plt.show(block=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
